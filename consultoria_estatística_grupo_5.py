# -*- coding: utf-8 -*-
"""Consultoria Estatística - Grupo 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuw04ykPqBE-4x_ciexMqga0zVXLFexa

# SME0882 Consultoria Estatística - Grupo 5: Sucateamento de Componentes Eletrônicos

A MSR é uma operadora logística que administra uma malha ferroviária de 1643km dos estados de Minas Gerais, Rio de Janeiro e São Paulo, região que concentra cerca da metade do PIB brasileiro.

Quase 20% de tudo o que o Brasil exporta e um terço de toda a carga transportada por trens no país passam pelos trilhos da MSR, que é uma das maiores ferrovias de carga do mundo, com produção cerca de 4 vezes superior àquela registrada em 1990.

Hoje em dia não existem parâmetros bem definidos objetivamente para a determinação do estado de sucateamento de uma peça pertencente aos equipamentos dos trens da ferrovia, podendo levar a uma perda considerável de tempo e dinheiro, aumentando também o risco de acidentes no transporte das cargas. Seria ideal que a determinação do estado de uma peça pudesse ser feita a partir de algum algoritmo, de modo a poupar uma boa parte gasta durante a análise técnica, feita pelos profissionais devidos.

**Objetivo:** Criar um sistema/plataforma que auxilie na tomada de decisão sobre sucatear ou retirar de circulação o item, quando inserido o seu número de série por um colaborador, baseada nos critérios abaixo
* Identificar a taxa de retorno;
* Identificar as causas do retorno (modo de falha, escrito em forma de texto livre)
* Identificar o reparo realizado
"""

import numpy as np
import pandas as pd
import seaborn as sns

from matplotlib import pyplot as plt
import matplotlib as mpl

COLOR = 'white'
mpl.rcParams['text.color'] = COLOR
mpl.rcParams['axes.labelcolor'] = COLOR
mpl.rcParams['xtick.color'] = COLOR
mpl.rcParams['ytick.color'] = COLOR

# url='https://docs.google.com/spreadsheets/d/18UaWDBuRkarVx5Q2TzEs8Zimai7n1AkG/edit?usp=sharing&ouid=113959658708931394981&rtpof=true&sd=true'
# Dados com a coluna Status arrumada
url = "https://docs.google.com/spreadsheets/d/1jkHdODcw5NOvggBcWnOgmDhWntFwNjHM/edit?usp=sharing&ouid=113959658708931394981&rtpof=true&sd=true"
url='https://drive.google.com/uc?id=' + url.split('/')[-2]
print(url)
df = pd.read_excel(url)

print("Shape:", df.shape)
df.head(2)

"""## Análise Descritiva"""

df.isna().sum()

ind_no_na = np.where(~df.iloc[:,:-1].isna().any(axis = 1))[0]
df = df.iloc[ind_no_na,:]
print("Shape:", df.shape)

"""A princípio já temos um problema bem evidente: o número de observações faltantes é muito alto, principalmente na variável resposta que gostaríamos de predizer, que conta com $46,59\%$ de valores faltantes. Podemos tentar realizar certas medidas de distância entre essas observações incertas e outras observações já categorizadas como "Baixado" ou "Recuperado" de modo a inserirmos manualmente esses resultados.

Para as demais colunas com observações faltantes, no momento convém retirarmos os dados da análise

### Confiabilidade da coluna Status

Além do excessivo número de observações faltantes na coluna Status (observações)

## Análise de Sobrevivência

Façamos uma breve análise de sobrevivência da idade dos equipamentos no momento da manutenção. Para a construção desses tempos fazemos a subtração da data de manutenção pela data de fabricação do produto, ambas contidas no conjunto de dados.
"""

dates = df.loc[:, ["DataManut","DataFabric"]]
dates["time"] = (dates.DataManut - dates.DataFabric).dt.days
dates.head(2)

print(dates.loc[dates.time < 0, :].shape[0], "observações com tempos negativos. Removendo-as...")
df = df.loc[dates.time > 0, :]
dates = dates.loc[dates.time > 0, :]

times = dates["time"]/30
status = pd.Series( np.repeat(1, len(times)) )

"""Tendo o tempo de vida do equipamento desde a data de fabricação até a data de manutenção vejamos como essa variável se comporta."""

!pip install lifelines &> /dev/null
!pip install survive &> /dev/null

import lifelines as ll
import survive

df = df.reset_index()

codigos = np.array([df.Codigo_Descricao[j].split(" - ")[0] for j in range(df.shape[0])])
produtos = np.array([df.Codigo_Descricao[j].split(" - ")[1] for j in range(df.shape[0])])
id_codigo_produto = dict(zip(codigos, produtos))

kmf = ll.KaplanMeierFitter()

kmf.fit(times, status)

plt.rcParams['axes.facecolor'] = '#031a2fff'
fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (12,6), facecolor = '#031a2fff')

kmf.plot(color = "white", label = "")
plt.setp(ax.spines.values(), color="white")
plt.xlabel("Tempo", fontsize = 16)
plt.ylabel("Sobrevivência", fontsize = 16)
plt.legend(fontsize = 16)
plt.show()

kmfits = []
for codigo in id_codigo_produto.keys():
  kmf = ll.KaplanMeierFitter(label = codigo)
  pos = np.where( df.Codigo == codigo )[0]
  kmf.fit( times.iloc[pos], status.iloc[pos] )
  kmfits.append( kmf )

plt.rcParams['axes.facecolor'] = '#031a2fff'
fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (12,6), facecolor = '#031a2fff')
plt.setp(ax.spines.values(), color="white")

for kmf in kmfits:
  kmf.plot_survival_function(ci_show=False, ax = ax )
  plt.xlabel("Tempo", fontsize = 16)
  plt.ylabel("Sobrevivência", fontsize = 16)

plt.legend(fontsize = 14)
plt.show()

from lifelines import WeibullAFTFitter, LogNormalAFTFitter, LogLogisticAFTFitter
import scipy as scp
from scipy.stats import kstest, weibull_min

lifetimes = pd.DataFrame({'time': list(times), 'status': list(status), 'codigo': list(codigos)})
lifetimes = pd.get_dummies(lifetimes, drop_first=True)
lifetimes.head(2)

aft = WeibullAFTFitter(model_ancillary = False)
aft.fit(lifetimes, 'time', 'status')

aft.params_

scale = np.exp(-1.1418)
k_r = 1/scale
np.log(k_r)

scale

aft.params_ = pd.Series([-0.1142,-0.1600, 0.1441, -0.2848,-0.6100, 4.8112, np.log(k_r)])

aft.params_

def get_weibull_survival(lifetimes, aft):
  params = np.array(list(aft.params_))
  covariates = lifetimes.iloc[:,2:]
  S = []
  for j in range(covariates.shape[0]):
    eta_lambda = np.sum( covariates.iloc[j,:] * params[:5] ) + params[5]
    # eta_rho = np.sum( covariates.iloc[j,:] * params[7:13] ) + params[13]
    eta_rho = params[6]

    lambda_ = np.exp( eta_lambda )
    rho_ = np.exp( eta_rho )

    S.append( 1 - weibull_min.cdf(lifetimes.iloc[j,0], c = rho_, scale = lambda_) )
  return np.array(S)

def plot_weibull_survival(lifetimes, aft):

  plt.rcParams['axes.facecolor'] = '#031a2fff'
  fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (12,6), facecolor = '#031a2fff')
  plt.setp(ax.spines.values(), color="white")

  colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

  Times, S = [], []
  j = 0
  for c in lifetimes.columns[2:]:
    pos = np.where(lifetimes.loc[:,c] == 1)[0] # Índice das observações do código correspondente da coluna
    t = lifetimes.iloc[pos,0]
    s = get_weibull_survival(lifetimes.iloc[pos,:], aft)
    Times.append( t )
    S.append( s )

    kmf = ll.KaplanMeierFitter(label = c)
    kmf.fit( times.iloc[pos], status.iloc[pos] )
    kmf.plot_survival_function(ci_show=False, ax = ax, color=colors[j])
    plt.xlabel("Tempo", fontsize = 16)
    plt.ylabel("Sobrevivência", fontsize = 16)

    ax.plot(t, s, color=colors[j])

    j += 1

  plt.legend(fontsize = 14)
  plt.show()

lifetimes_sort = lifetimes.sort_values(by = "time")
t = lifetimes_sort.iloc[:,0]
S = get_weibull_survival(lifetimes_sort, aft)

plot_weibull_survival(lifetimes_sort, aft)

"""Podemos perceber uma distribuição relativamente comportada. Façamos um teste com alguns estimadores paramétricos. Consideremos que os tempos de vida do equipamento apresentam distribuições Weibull e Log Normal."""

from lifelines.fitters.weibull_fitter import WeibullFitter
from lifelines.fitters.log_normal_fitter import LogNormalFitter

wf = WeibullFitter()
lf = LogNormalFitter()

wf.fit(times, status)
lf.fit(times, status)

wfp = np.round(wf.params_, 2)
lfp = np.round(lf.params_, 2)

fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (12,6), facecolor = '#031a2fff')
plt.setp(ax.spines.values(), color="white")

plt.step(list(kmf.timeline), kmf.survival_function_, color = "white", label = "Estimador de Kaplan-Meier")
plt.plot(wf.timeline, wf.survival_function_, color = "red",
         label = r"Weibull ($\lambda$ = {}, k = {})".format(wfp[0], wfp[1]))
plt.plot(lf.timeline, lf.survival_function_, color = "cyan",
         label = r"Lognormal ($\mu$ = {}, $\sigma$ = {})".format(lfp[0], lfp[1]))
plt.xlabel("Tempo", fontsize = 16)
plt.ylabel("Sobrevivência", fontsize = 16)
plt.legend(fontsize = 16)
plt.show()

"""Claramente pelo gráfico vemos um melhor ajuste do modelo Weibull. Para podermos concluir objetivamente basta observarmos que os valores das estatísticas AIC e BIC da distribuição Weibull são muito inferiores às estatísticas do modelo Log Normal."""

R = pd.DataFrame({"AIC": [wf.AIC_, lf.AIC_], "BIC": [wf.BIC_, lf.BIC_]})
R.index = ["Weibull", "Lognormal"]
R

"""Além de concluir que o modelo Weibull é o melhor, podemos ter o interesse em determinar se de fato essa distribuição é estatisticamente adequada aos dados. Para verificarmos a confiança no modelo Weibull, façamos o teste de Komolgorov-Smirnov."""

import scipy as scp
from scipy.stats import kstest, weibull_min

# Para obtermos a fdp da Weibull: k/lambda (x / lambda)^(k-1) exp{ -(x/lambad)^k }
# basta tomarmos weibull_min com c = k e scale = lambda

lambda_ = wf.params_[0]
k = wf.params_[1]

kstest(times, weibull_min.cdf, args = (k, 0, lambda_)) # args: c = k, loc = 0, scale = lambda_

"""Como podemos ver obtivemos um p-valor de 0.1315, que ao nível de significância $\alpha = 0.05$ não representa indícios suficientes para rejeitarmos a hipótese de que os tempos de vida seguem uma distribuição Weibull com os parâmetros estimados pelo estimador. A partir deste momento podemos usar como resultado que sendo $T$ o tempo da fabricação do equipamento até a data que ele foi enviado para a manutenção em meses, sua distribuição será dada por

\begin{equation*}
    T \sim \text{Weibull}(k = 2.7183, \lambda = 108.9899).
\end{equation*}

### Taxa de Falha

Agora que temos uma distribuição satisfatoriamente estabelecida, podemos obter medidas dos dados que podem ser de interesse na análise dos tempos de vida dos equipamentos. Podemos, por exemplo, tomar os valores das taxas de falha dos diferentes grupos de equipamentos de modo a verificar os valores limiares ideais que dividem em essência os produtos a serem consertados e os produtos a serem baixados.

A função taxa de falha da distribuição $\text{Weibull}(k, \lambda)$ pode ser escrita como
\begin{equation*}
    h(t) = \dfrac{k}{\lambda^k} t^{k - 1}.
\end{equation*}
Dada uma peça da manutenção, podemos obter esse valor como uma medida útil para determinarmos se a mesma ainda pode ser reutilizada ou descartada. O gráfico da função taxa de falha para a distribuição $\text{Weibull}$ estimada dos dados é dado a seguir.
"""

x = np.arange(0, 250, 0.1)

fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (12,6), facecolor = '#031a2fff')
plt.setp(ax.spines.values(), color="white")

plt.plot(x, k/lambda_**k * x**(k-1), color="white")
plt.title("Função Taxa de Falha de T")
plt.xlabel("Tempo", fontsize = 16)
plt.ylabel("h(t)", fontsize = 16)
plt.show()

"""De fato, perceba que a função taxa de falha estimada é crescente, indicando que a probabilidade "instantânea" de falha do produto, dada por
\begin{equation*}
    h(t) = \lim_{\Delta t \rightarrow 0}\dfrac{P(t \leq T < t + \Delta t | T \geq t)}{\Delta t} = \dfrac{f(t)}{S(t)},
\end{equation*}
aumenta a medida que o tempo de vida aumenta, modelando um certo desgaste da peça ao longo do tempo.

###
"""

